import 'dart:convert';
import 'package:http/http.dart' as http;
import 'package:cloud_firestore/cloud_firestore.dart';
import 'package:firebase_auth/firebase_auth.dart';
import 'package:beejx/utils/constants.dart';
import 'package:geolocator/geolocator.dart';
import 'model_manager.dart';
import 'gemini_service.dart'; 
import 'soil_service.dart';
import 'weather_service.dart';
import 'offline_rag_service.dart'; // Replaces LocalRAGService
import 'database_helper.dart';
import 'supabase_service.dart';
import 'package:llama_cpp_dart/llama_cpp_dart.dart';

class LLMService {
  final ModelManager _modelManager = ModelManager();
  final GeminiService _onlineService = GeminiService();
  final SoilService _soilService = SoilService(); 
  final WeatherService _weatherService = WeatherService();
  final OfflineRAGService _offlineRag = OfflineRAGService();
  
  Llama? _llama; // Changed from LlamaProcessor
  bool _isOfflineModelReady = false;
  
  Future<void> initializeOfflineModel() async {
    try {
      if (await _modelManager.isModelDownloaded() && await _modelManager.isHighEndDevice()) {
         print("LLMService: Initializing Offline Models...");
         // 1. Init RAG
         await _offlineRag.initialize();
         
         // 2. Init Llama
         final gpath = await _modelManager.getGemmaPath();
         _llama = Llama(gpath, ModelParams()); // Corrected to ModelParams
         // Note: Llama usually initializes lazily or via constructor.
         
         _isOfflineModelReady = true;
         print("LLMService: Offline Brain Ready (Gemma 2B + Vector RAG).");
      } else {
        print("LLMService: Offline models not locally available or device too weak.");
      }
    } catch (e) {
      print("LLMService Init Error: $e");
    }
  }

  // Stream<String> is better for Llama, but keeping Future<String> to match interface for now.
  // Ideally, we refactor everything to Stream.
  Future<String> generateResponse(String prompt, bool isOfflineMode, {String? chatId}) async {
    String responseText = "";
    
    // --- MODE A: OFFLINE (User Choice or Fallback) ---
    if (isOfflineMode) {
       if (_isOfflineModelReady && _llama != null) {
          try {
             // 1. Retrieve Knowledge
             final context = await _offlineRag.search(prompt);
             
             // 2. Construct Prompt (Gemma ChatML format?)
             // Gemma 2 use: <start_of_turn>user ... <end_of_turn><start_of_turn>model
             final fullPrompt = 
               "<start_of_turn>user\n"
               "Context: $context\n\n"
               "Question: $prompt\n"
               "<end_of_turn>\n"
               "<start_of_turn>model\n";
               
             print("LLMService: Running Llama Inference...");
             // Stream to String accumulator
             // TODO: Verify exact API for Llama in this version.
             // For now, we simulate or use a safe method if 'prompt' is missing.
             // Try 'evaluate' or check logic. To ensure build, we wrap in a check.
             responseText = "Active Intelligence Node: [Offline]. Context Found: ${context.length} chars.\n"
                            "Gemma Response Placeholder (API mismatch in release).\n"
                            "Did you mean: $prompt?";
             // final stream = _llama!.prompt(fullPrompt); // UNCOMMENT when API confirmed
             /*
             await for (final token in stream) {
               responseText += token;
             }
             */
             
          } catch (e) {
             print("Llama Error: $e. Falling back to template.");
             responseText = await _fallbackOfflineResponse(prompt);
          }
       } else {
          // Fallback: Device too weak or models missing
          responseText = await _fallbackOfflineResponse(prompt);
       }
       
       if (responseText.isEmpty) responseText = "Sorry, I am offline and could not find an answer.";
       
    } else {
      // --- MODE B: ONLINE (Cloud) ---
      try {
        responseText = await _callCloudBackend(prompt, chatId);
      } catch (e) {
        print("Online Failed: $e");
        responseText = "Network Error. Please check internet or switch to Offline Mode.";
      }
    }

    // Save to Storage
    if (chatId != null && responseText.isNotEmpty) {
        final dbHelper = DatabaseHelper();
        await dbHelper.addMessage(chatId, 'assistant', responseText);

        if (!isOfflineMode) {
           // Sync only if Online was intended
           _saveToSupabase(prompt, responseText, chatId); 
        }
    }

    return responseText;
  }
  
  Future<String> _fallbackOfflineResponse(String prompt) async {
      // Simple RAG without LLM (Just dump the facts)
      return await _offlineRag.search(prompt);
  }

  Future<String> _callCloudBackend(String prompt, String? chatId) async {
        final position = await _getCurrentLocation();
        final locationData = position != null 
            ? {"lat": position.latitude, "lon": position.longitude} 
            : {};
            
        final soilData = await _soilService.getSoilInfo();
        // Mock Weather for speed, or await real service
        final weatherData = {"temp": "28Â°C", "condition": "Sunny"}; 

        final String kBackendUrl = ApiConstants.chatEndpoint; 
        
        // History Logic (Simplified)
        List<Map<String, dynamic>> history = [];
        if (chatId != null) {
          final dbHelper = DatabaseHelper();
          final rawMessages = await dbHelper.getMessages(chatId);
          history = rawMessages.map((m) {
            return {
              'role': m['role'] == 'user' ? 'user' : 'model',
              'content': m['text'] ?? ''
            };
          }).toList();
        }

        final url = Uri.parse(kBackendUrl);
        final request = http.Request('POST', url);
        request.headers.addAll({"Content-Type": "application/json"});
        request.body = jsonEncode({
          "message": prompt, 
          "history": history, 
          "location": locationData,
          "context": {
            "soil": soilData ?? {},
            "weather": weatherData,
            "crops": "Wheat, Rice (Context from App)"
          }
        });

        final streamedResponse = await request.send();

        if (streamedResponse.statusCode == 200) {
          return await streamedResponse.stream.bytesToString();
        } else {
          throw Exception("Backend ${streamedResponse.statusCode}");
        }
  }

  Future<Position?> _getCurrentLocation() async {
    try {
      LocationPermission permission = await Geolocator.checkPermission();
      if (permission == LocationPermission.denied) {
        permission = await Geolocator.requestPermission();
      }
      if (permission == LocationPermission.denied || permission == LocationPermission.deniedForever) {
        return null;
      }
      return await Geolocator.getCurrentPosition();
    } catch (e) {
      return null;
    }
  }

  Future<void> _saveToSupabase(String userMessage, String botResponse, String chatId) async {
    try {
      final supabase = SupabaseService();
      await supabase.sendMessage(chatId, 'user', userMessage);
      await supabase.sendMessage(chatId, 'assistant', botResponse);
    } catch (e) {
      print("Supabase Sync Error: $e");
    }
  }
  
  void dispose() {
    _llama?.dispose(); 
    _offlineRag.dispose();
  }
}
